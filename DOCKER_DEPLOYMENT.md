# TranscrevAI - Docker Deployment Guide

ðŸš€ **Complete containerized Portuguese Brazilian transcription and diarization system**

## Quick Start (1-Command Deploy)

Anyone with Docker can test TranscrevAI immediately:

```bash
# Clone and run TranscrevAI
git clone <repository-url>
cd TranscrevAI_windows
docker-compose up -d
```

**Access**: http://localhost:8000

## Features

âœ… **Universal Compatibility**: Windows/Linux/macOS (including Silicon)
âœ… **Portuguese Brazilian Optimized**: Specialized for PT-BR transcription
âœ… **Auto SRT Download**: Automatic file download with path display
âœ… **Live Recording**: MP4 or WAV format choice
âœ… **CPU-Only Optimized**: INT8 quantization for efficient CPU processing
âœ… **Minimum Specs**: 4 cores, 8GB RAM

## System Requirements

### Minimum Requirements
- **CPU**: 4 cores
- **RAM**: 8GB DDR3
- **Storage**: 5GB free space
- **OS**: Windows 10/11, Linux, macOS

### Recommended Requirements
- **CPU**: 8+ cores
- **RAM**: 16GB
- **Storage**: 10GB SSD

## Installation Methods

### Method 1: Production Deployment (Recommended)

```bash
# Production deployment with persistent data
docker-compose up -d

# Check status
docker-compose ps

# View logs
docker-compose logs -f transcrevai
```

### Method 2: Development Mode

```bash
# Development with hot reload
docker-compose -f docker-compose.yml -f docker-compose.dev.yml up -d
```

### Method 3: Direct Docker Run

```bash
# Build image
docker build -t transcrevai .

# Run container
docker run -d \
  --name transcrevai \
  -p 8000:8000 \
  -v $(pwd)/data:/app/data \
  transcrevai
```

## Usage Guide

### 1. Upload Audio Files

```bash
# Test upload via curl
curl -X POST -F "file=@your-audio.wav" http://localhost:8000/upload
```

### 2. Live Recording

1. Access http://localhost:8000
2. Choose MP4 or WAV format
3. Start recording
4. Stop when finished
5. SRT file automatically downloads

### 3. WebSocket Real-time Updates

```javascript
const ws = new WebSocket('ws://localhost:8000/ws/SESSION_ID');
ws.onmessage = (event) => {
    const data = JSON.parse(event.data);
    console.log('Progress:', data.progress);
};
```

## API Endpoints

- **Upload**: `POST /upload` - Upload audio files
- **Health**: `GET /health` - Health check
- **WebSocket**: `ws://localhost:8000/ws/{session_id}` - Real-time updates
- **Download**: `GET /download/srt/{session_id}` - Download SRT files

## Configuration

### Environment Variables

```bash
# Production settings
ENVIRONMENT=production
LOG_LEVEL=INFO
DATA_DIR=/app/data
WHISPER_MODEL_DIR=/app/data/models/whisper
```

### Volume Mappings

```yaml
volumes:
  - ./data/uploads:/app/data/uploads      # Uploaded files
  - ./data/transcripts:/app/data/transcripts  # Generated transcripts
  - ./data/srt:/app/data/srt              # SRT files
  - ./data/recordings:/app/data/recordings    # Live recordings
```

## Performance Optimization

### FASE 10: Production-Ready Optimizations

#### Model Pre-Download (Build Time)

Models are pre-downloaded during Docker build to eliminate cold start download time:

- **Faster-Whisper medium model** (~1.5GB) cached in `/root/.cache/huggingface`
- **Impact**: Cold start reduced from 209s â†’ 20-25s (eliminates model download time)
- **Implementation**: Automatic during `docker build`

#### Lazy Unload Memory Management (Sprint 2 Dia 2.1)

Configure `MODEL_UNLOAD_DELAY` environment variable for intelligent memory management:

```bash
# Default: Unload after 60s idle (recommended)
docker run -e MODEL_UNLOAD_DELAY=60 transcrevai

# Disable lazy unload (keep model loaded)
docker run -e MODEL_UNLOAD_DELAY=0 transcrevai

# More aggressive (30s idle)
docker run -e MODEL_UNLOAD_DELAY=30 transcrevai

# Or in docker-compose.yml:
environment:
  - MODEL_UNLOAD_DELAY=60
```

**How it works**:
- Timer resets on each transcription request
- Model unloads only after configured idle period
- Zero overhead during continuous use
- Automatic reload on next request

**Trade-offs**:
- âœ… **Pros**: Zero overhead in continuous use, frees ~400-500MB when idle
- âœ… **Best of both worlds**: Fast warm start + memory efficiency
- âš–ï¸ **Use case**: Any usage pattern (automatic adaptation)
- ðŸŽ¯ **Default**: `60` (unload after 60s idle)

#### Batch Processing (Sprint 2 Dia 2.2)

Process multiple audio files simultaneously for 12.5x speedup:

```python
# Enable batch mode
engine.enable_batch_mode()

# Process multiple files
results = engine.transcribe_batch([
    "audio1.wav",
    "audio2.wav",
    "audio3.wav"
])
```

**Performance**:
- **12.5x faster** than sequential processing
- Processes 16 audio chunks simultaneously
- Ideal for: Batch jobs, playlist transcription, background processing

#### Shared Memory Multiprocessing (Sprint 2 Dia 2.3)

Optimized for large audio files (>100MB):

```python
from src.performance_optimizer import process_with_shared_memory

# Avoids pickling overhead
result = process_with_shared_memory(audio_data, worker_func)
```

**Performance**:
- **20-30% faster** multiprocessing for large files
- Eliminates pickling overhead
- Automatic fallback to normal processing if shared memory fails

**Performance Targets**:
- **Cold start**: â‰¤2.8x real-time (with model pre-download)
- **Warm start**: â‰¤1.0x real-time (models cached in memory)
- **Batch processing**: â‰¤0.08x real-time (12.5x speedup)

### CPU-Only Optimization

The system uses optimized CPU-only processing with INT8 quantization:

- **faster-whisper**: CTranslate2 backend with INT8 quantization
- **Adaptive beam size**: Optimized for audio duration
- **VAD filtering**: Silero VAD with automatic fallback
- **Memory efficient**: Progressive model loading and optional auto-unload

### Memory Management

- **Peak Usage**: <2GB for 8GB systems
- **Model Loading**: Progressive loading for browser safety
- **Emergency Mode**: Automatic fallback if memory pressure
- **Lazy Unload**: Intelligent memory recycling (~400-500MB freed after idle)
- **Shared Memory**: Zero-copy multiprocessing for large files

## Troubleshooting

### Common Issues

**1. Port Already in Use**
```bash
# Check what's using port 8000
docker ps
sudo lsof -i :8000

# Use different port
docker-compose up -d -p 8001:8000
```

**2. Memory Issues**
```bash
# Check memory usage
docker stats transcrevai

# Restart with more memory
docker-compose down
docker-compose up -d
```

**3. Performance Issues**
```bash
# Check CPU usage
docker stats transcrevai

# Check model loading status
docker-compose logs transcrevai | grep "model loaded"

# Verify INT8 quantization is active
docker-compose logs transcrevai | grep "INT8"
```

### Health Checks

```bash
# Check container health
docker-compose ps

# Test health endpoint
curl http://localhost:8000/health

# View detailed logs
docker-compose logs --tail=50 transcrevai
```

## Production Deployment

### Security Considerations

1. **Firewall**: Only expose necessary ports
2. **SSL/TLS**: Use reverse proxy for HTTPS
3. **Resource Limits**: Set appropriate memory/CPU limits
4. **Monitoring**: Implement health monitoring

### Reverse Proxy Example (Nginx)

```nginx
server {
    listen 80;
    server_name your-domain.com;

    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    location /ws/ {
        proxy_pass http://localhost:8000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
    }
}
```

## Testing & Validation

### Automated Testing

```bash
# Run health check
curl -f http://localhost:8000/health

# Test upload functionality
curl -X POST -F "file=@test-audio.wav" http://localhost:8000/upload

# Check response time
curl -w "@curl-format.txt" -o /dev/null -s http://localhost:8000/health
```

### Performance Benchmarks

Expected performance on minimum specs (4 cores, 8GB RAM) with CPU-only INT8:

- **Startup Time**: <30s (including model loading with pre-download)
- **Cold Start Processing**: â‰¤2.8x real-time
- **Warm Start Processing**: â‰¤1.0x real-time
- **Memory Usage**: <2GB peak
- **Accuracy**: â‰¥90% for PT-BR transcription and diarization

## Support & Maintenance

### Container Management

```bash
# Update container
docker-compose pull
docker-compose up -d

# Backup data
docker run --rm -v transcrevai_data:/data -v $(pwd):/backup busybox tar czf /backup/transcrevai-backup.tar.gz /data

# Restore data
docker run --rm -v transcrevai_data:/data -v $(pwd):/backup busybox tar xzf /backup/transcrevai-backup.tar.gz -C /
```

### Monitoring

```bash
# Resource usage
docker stats transcrevai

# Log monitoring
docker-compose logs -f --tail=100 transcrevai

# Health monitoring
watch -n 30 'curl -s http://localhost:8000/health'
```

## License & Contributing

TranscrevAI is designed for Portuguese Brazilian transcription and diarization with cross-platform Docker deployment capabilities.

For issues or contributions, please refer to the project repository.

---

**Ready to test TranscrevAI? Run `docker-compose up -d` and visit http://localhost:8000**