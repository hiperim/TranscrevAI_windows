# üß† TRANSCREVAI MEMORY OPTIMIZATION - COMPREHENSIVE IMPLEMENTATION PLAN

**Generated by Claude + Gemini Analysis | Following CLAUDE.md Strategy**

---

## üìä **CURRENT STATE ANALYSIS**
- **Memory Available**: 1.0GB
- **Memory Required**: 1.5GB (causing failures)
- **Target**: Reduce to ~0.3-0.6GB requirement
- **Architecture**: CPU-only, multiprocessing, INT8 quantization

---

## üéØ **IMPLEMENTATION LEVELS**

### üü¢ **LEVEL 1: EASY (Quick Wins - 1-2 Days)**
*Low risk, immediate benefits, no architectural changes*

#### **1.1 Systematic Garbage Collection**
- **Memory Reduction**: 15-25%
- **Risk Level**: LOW ‚úÖ
- **Files**: `src/transcription.py`, `src/diarization.py`
- **Implementation**:
  ```python
  import gc

  # In _process_transcription_request finally block:
  finally:
      gc.collect()  # Force cleanup after processing

  # After heavy operations:
  del features_scaled, features, mfccs
  gc.collect()
  ```
- **Testing**: Monitor memory usage in processing loop - should return to baseline

#### **1.2 Consolidate Lazy Imports**
- **Memory Reduction**: 20-30% startup memory
- **Risk Level**: LOW ‚úÖ
- **Files**: `src/production_optimizer.py`, `src/audio_processing.py`
- **Implementation**:
  ```python
  # Replace direct imports with lazy loading
  # Remove: import torch, librosa, onnxruntime
  # Add: torch = await lazy_imports.import_torch()
  ```
- **Testing**: Measure startup time reduction, verify functionality

#### **1.3 Memory-Efficient Dataclasses**
- **Memory Reduction**: 5-15% object overhead
- **Risk Level**: LOW ‚úÖ
- **Files**: `src/performance_optimizer.py`, `src/production_optimizer.py`
- **Implementation**:
  ```python
  @dataclass(slots=True)  # Requires Python 3.10+
  class ProcessConfig:
      # existing fields
  ```
- **Testing**: Create thousands of objects, measure memory difference

---

### üü° **LEVEL 2: MEDIUM (Core Improvements - 3-5 Days)**
*Moderate risk, significant impact, requires careful integration*

#### **2.1 Memory-Mapped Audio Loading**
- **Memory Reduction**: 40-60% for large files
- **Risk Level**: MEDIUM ‚ö†Ô∏è
- **Files**: `src/diarization.py`, `src/transcription.py`
- **Implementation**:
  ```python
  # Replace: sf_mod.read(audio_file)
  # With: OptimizedAudioProcessor.memory_mapped_audio_load(audio_file)

  # Use memory-mapped NumPy arrays for large files
  audio_data = np.memmap(audio_file, mode='r')
  ```
- **Testing**: Process 1GB+ audio files without memory crashes
- **Potential Issues**:
  - Audio boundary artifacts with chunking
  - Context loss between segments
  - Synchronization issues with overlapping chunks

#### **2.2 Unified Model Caching**
- **Memory Reduction**: 30-50% during idle time
- **Risk Level**: MEDIUM ‚ö†Ô∏è
- **Files**: `src/production_optimizer.py`, `src/file_manager.py`, `src/transcription.py`
- **Implementation**:
  ```python
  # Consolidate multiple cache managers into single ModelCacheManager
  # Implement intelligent eviction based on usage and memory pressure
  model = await production_optimizer.get_model_lazy(model_name)
  ```
- **Testing**: Verify model loading/unloading, eviction policies
- **Potential Issues**:
  - Cold start latency on first requests
  - Cache invalidation complexity
  - Concurrent access during loading

#### **2.3 Context-Aware Cache Eviction**
- **Memory Reduction**: 20-40% cache efficiency
- **Risk Level**: MEDIUM ‚ö†Ô∏è
- **Files**: `src/production_optimizer.py`, `src/performance_optimizer.py`
- **Implementation**:
  ```python
  # Replace simple LRU with scoring system:
  # score = (priority * w1) - (time_since_last_use * w2) - (model_size_mb * w3)
  eviction_score = calculate_eviction_priority(model_info)
  ```
- **Testing**: Simulate memory pressure, verify intelligent eviction
- **Potential Issues**:
  - Complex scoring logic
  - Memory spikes during model loading
  - State management complexity

---

### üî¥ **LEVEL 3: HARD (Architectural Changes - 1-2 Weeks)**
*High risk, maximum impact, requires major refactoring*

#### **2.4 Memory Pool Management**
- **Memory Reduction**: 20-40%
- **Risk Level**: MEDIUM ‚ö†Ô∏è
- **Files**: `src/audio_processing.py`, `src/performance_optimizer.py`
- **Implementation**: Pre-allocate fixed-size buffers, reuse memory blocks
- **Current**: DynamicMemoryManager exists but can be optimized
- **Testing**: Verify pool allocation/deallocation, measure memory reuse efficiency
- **Potential Issues**:
  - Pool exhaustion during peak usage
  - Memory waste from pre-allocated pools
  - Size miscalculation leading to frequent allocations
  - Complex pool management logic

---

### üî¥ **LEVEL 3: HARD (Architectural Changes - 1-2 Weeks)**
*High risk, maximum impact, requires major refactoring*

#### **3.1 Automated INT8 Quantization Pipeline**
- **Memory Reduction**: 50-75% model size with automation
- **Risk Level**: MEDIUM-HIGH ‚ö†Ô∏è
- **Files**: `src/models.py`, `src/production_optimizer.py`
- **Implementation**:
  ```python
  # Automatic quantization workflow:
  def get_model():
      if exists("model_int8.onnx"):
          return load_int8_model()
      elif exists("model_fp32.onnx"):
          return convert_and_load_int8()
      else:
          download_model()
          return convert_and_load_int8()
  ```
- **Testing**: Compare FP32 vs INT8 accuracy, verify auto-conversion
- **Potential Issues**:
  - Accuracy loss (1-3% for Portuguese audio)
  - Hardware dependency for performance gains
  - Quantization bugs harder to debug
  - Model-specific sensitivity issues

#### **3.2 Switch to faster-whisper (Complete Rewrite)**
- **Memory Reduction**: 60-70% less RAM usage
- **Risk Level**: MEDIUM (Major refactoring required but proven stable) ‚ö†Ô∏è
- **Files**: Complete system refactoring
- **Implementation**: Drop-in replacement for openai-whisper
- **Benefits**: Same accuracy, 4x faster, significantly less memory
- **Testing**: Complete system validation, accuracy comparison
- **Potential Issues**:
  - Dependency changes requiring complete rewrite
  - API differences from openai-whisper
  - Ecosystem compatibility issues
  - Documentation gaps

#### **3.3 Profile-Guided Optimization**
- **Memory Reduction**: Variable (identifies actual bottlenecks)
- **Risk Level**: MEDIUM üü°
- **Files**: Determined by profiling results
- **Implementation**:
  ```python
  # Use professional profiling:
  # py-spy record -o profile.svg -- python main.py
  # scalene main.py

  # Target identified hotspots with:
  # - Vectorized NumPy operations
  # - Function memoization
  # - Cython for extreme bottlenecks
  ```
- **Testing**: Establish baseline metrics, measure optimization impact
- **Potential Issues**:
  - Profiling overhead (5-15%)
  - Large diagnostic data volume
  - Analysis complexity

---

## üéØ **CORRECTED TECHNIQUE COUNT: 9 TOTAL**

**Level 1 (EASY)**: 3 techniques - Quick wins with minimal risk
**Level 2 (MEDIUM)**: 4 techniques - Core improvements with moderate risk
**Level 3 (HARD)**: 2 techniques - Major architectural changes

**Note**: Zero-Copy IPC was removed per user request due to Windows compatibility concerns.

---

#### **3.4 Graceful Degradation System**
- **Memory Reduction**: Prevents crashes, maintains functionality
- **Risk Level**: HIGH üö®
- **Files**: `src/performance_optimizer.py`, all major components
- **Implementation**:
  ```python
  # Multi-level degradation:
  memory_state = ResourceManager.get_system_state()

  if memory_state == "WARNING":
      # Reduce quality: simple diarization, smaller chunks
      use_simple_algorithm = True
  elif memory_state == "CRITICAL":
      # Reject new requests, unload models
      return {"error": "server_busy", "retry_after": 60}
  ```
- **Testing**: Artificially consume RAM, verify degradation levels
- **Potential Issues**:
  - Complex state management
  - User experience degradation
  - Difficult to test all scenarios

---

## üéØ **RECOMMENDED IMPLEMENTATION ORDER**

### **Week 1: Foundation (EASY)**
1. **Day 1**: Implement systematic garbage collection
2. **Day 2**: Consolidate lazy imports and add memory-efficient dataclasses

**Expected Result**: ~25-35% memory reduction

### **Week 2: Core Improvements (MEDIUM)**
1. **Days 3-4**: Memory-mapped audio loading
2. **Days 5-6**: Unified model caching system
3. **Day 7**: Context-aware cache eviction

**Expected Result**: ~50-65% memory reduction (should meet 1.0GB constraint)

### **Week 3-4: Advanced (HARD) - Optional**
1. **Week 3**: Automated INT8 pipeline and faster-whisper evaluation
2. **Week 4**: Profile-guided optimization and graceful degradation

**Expected Result**: ~70-75% memory reduction

---

## üìã **TESTING STRATEGY**

### **Continuous Testing Requirements**
1. **No Pylance Errors**: Run `mcp__ide__getDiagnostics` after each change
2. **Functional Testing**: Process q.speakers.wav after each implementation
3. **Memory Monitoring**: Use `psutil` to track memory usage
4. **Performance Benchmarking**: Measure processing time impact

### **Memory Testing Protocol**
```python
import psutil
import gc

def test_memory_usage():
    gc.collect()
    baseline = psutil.Process().memory_info().rss / 1024 / 1024

    # Run transcription
    process_audio("test_file.wav")

    gc.collect()
    peak = psutil.Process().memory_info().rss / 1024 / 1024

    print(f"Baseline: {baseline:.1f}MB, Peak: {peak:.1f}MB")
    return peak - baseline
```

---

## ‚ö†Ô∏è **RISK MITIGATION**

### **High-Risk Items (Level 3)**
- **Always implement in feature branches**
- **Extensive testing with Portuguese audio samples**
- **Rollback plan for each architectural change**
- **Monitor accuracy metrics (WER - Word Error Rate)**

### **Success Criteria**
- **Memory**: Reduce from 1.5GB to ‚â§1.0GB requirement
- **Accuracy**: <2% degradation in transcription quality
- **Performance**: <20% increase in processing time
- **Stability**: No crashes during 8-hour stress testing

---

## üéâ **EXPECTED FINAL RESULTS**

**Before Optimization**:
- ‚ùå Memory Required: 1.5GB
- ‚ùå Memory Available: 1.0GB
- ‚ùå Result: Processing failure

**After Full Implementation**:
- ‚úÖ Memory Required: 0.3-0.6GB (60-75% reduction)
- ‚úÖ Memory Available: 1.0GB
- ‚úÖ Result: Stable processing with room for growth

This comprehensive plan follows the CLAUDE.md strategy of using Gemini for analysis and Claude for implementation, ensuring gradual, tested improvements without introducing Pylance errors or system instability.