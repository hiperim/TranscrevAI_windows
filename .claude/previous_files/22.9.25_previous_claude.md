# Enhanced Claude Agent - Prompt Chaining Implementation
*Optimized for efficient Gemini tool integration and error reduction*

## Core Principles: Atomic Prompts & Sequential Validation

This implementation follows prompt chaining best practices: breaking complex tasks into atomic, focused prompts with built-in validation checkpoints and error recovery mechanisms.

---

## Phase 1: Initial Context Analysis Chain
*Execute these prompts sequentially for systematic project understanding*

### Step 1.1: Project Context Discovery
**Prompt Chain Entry Point**
```
Analyze compliance.txt first and ONLY compliance.txt:
- Extract core project requirements and constraints
- Identify must-follow coding standards and patterns
- List prohibited patterns or approaches
- Output: Structured summary with clear constraints list

VALIDATION CHECKPOINT: Does the output contain specific, actionable constraints? If not, re-analyze with focus on concrete rules.
```

### Step 1.2: Recent Changes Analysis
**Sequential Analysis - Execute only after Step 1.1 validation passes**
```
Now analyze fixes.txt and latest.txt in sequence:
- fixes.txt: What specific problems were recently solved? What patterns caused issues?
- latest.txt: What new features/implementations were added? What approaches worked?
- Cross-reference against compliance.txt constraints
- Output: Timeline of changes with pattern analysis

VALIDATION CHECKPOINT: Are the recent changes compatible with compliance constraints? Flag any conflicts.
```

### Step 1.3: Agent Context Optimization  
**Final Context Step - Execute only after previous steps validated**
```
Analyze .claude/ folder files to determine optimal interaction approach:
- What communication patterns work best with this codebase?
- What complexity level should be used for explanations?
- What tools/approaches are preferred?
- Output: Interaction guidelines and agent configuration

VALIDATION CHECKPOINT: Do the guidelines align with project complexity and user preferences?
```

---

## Phase 2: Gemini Tool Integration Chain
*Systematic approach to external tool usage with error prevention*

### Step 2.1: Pre-Analysis Validation
**Before Any Gemini Call**
```
MANDATORY CHECKPOINT SEQUENCE:
1. Is this task too complex for Claude's context window? (>100KB or >50 files)
2. Does this require codebase-wide pattern analysis?
3. Does this need verification across multiple directories?
4. If YES to any above → proceed to Gemini chain
5. If NO → handle with native Claude capabilities

ERROR PREVENTION: Never call Gemini for single file analysis under 10KB.
```

### Step 2.2: Gemini Query Preparation Chain
**Atomic Prompt Sequence for Tool Optimization**

#### 2.2.1: Scope Definition
```
Define the exact scope of analysis needed:
- Specific files/directories to include
- Exact question to be answered
- Expected output format
- Success criteria

VALIDATION: Can this be answered with the specified files? If uncertain, expand scope incrementally.
```

#### 2.2.2: Query Construction
```
Construct optimal Gemini command:
- Use @ syntax for file inclusion
- Choose between single files (@file.py) vs directories (@src/) vs full project (@./)
- Craft specific question that avoids ambiguity
- Include context from Phase 1 analysis

ERROR PREVENTION: Always verify paths exist before constructing @ syntax.
```

#### 2.2.3: Implementation Verification Queries
**Specialized query patterns for common verification tasks:**

```bash
# Pattern 1: Feature Implementation Check
gemini -p "@src/ @lib/ Is [SPECIFIC_FEATURE] fully implemented? Show evidence: files, functions, and integration points. List any missing components."

# Pattern 2: Security/Compliance Validation  
gemini -p "@src/ @tests/ Does the codebase follow [SPECIFIC_COMPLIANCE_RULE]? Show violations with file:line references and suggested fixes."

# Pattern 3: Architectural Pattern Analysis
gemini -p "@src/ @config/ What architectural patterns are used for [SPECIFIC_FUNCTIONALITY]? Show implementation examples and consistency across codebase."

# Pattern 4: Error Handling Coverage
gemini -p "@src/ @api/ Is error handling implemented for [SPECIFIC_MODULE]? List uncovered error scenarios and show try-catch patterns."
```

---

## Phase 3: Enhanced Triple-Validation Strategy
*Systematic approach to complex task validation with error recovery*

### Step 3.1: Multi-Angle Analysis Chain
**Execute three separate analyses in sequence:**

#### 3.1.1: Targeted Analysis
```bash
# Focus on specific component
gemini -p "@[target_file/directory] Analyze [problem] and provide brief summary of needed fixes. Focus on root cause."
```

#### 3.1.2: Integration Analysis  
```bash
# Focus on system interactions
gemini -p "@[related_files] How does [problem] affect connected components? Brief integration impact analysis."
```

#### 3.1.3: Architecture Analysis
```bash  
# Focus on overall system design
gemini -p "@[system_level_directories] What's the architectural approach to solve [problem]? High-level implementation strategy."
```

### Step 3.2: Validation Synthesis Chain
**Sequential validation of analysis consistency:**

#### 3.2.1: Consistency Check
```
Compare the three analysis results:
- Do they identify the same root cause?
- Are the proposed solutions compatible?
- Do they conflict with compliance.txt constraints?
- Score consistency: High/Medium/Low

DECISION POINT: 
- High consistency → Proceed to implementation
- Medium consistency → Re-analyze with focus on conflicting points  
- Low consistency → Escalate to user for guidance
```

#### 3.2.2: Solution Selection
```
Based on consistency analysis, select optimal approach:
- Prioritize compliance with propositions.txt
- Choose architectural fixes over quick patches
- Consider long-term maintainability
- Document reasoning for selection

VALIDATION: Does selected solution address all identified issues without creating new ones?
```

#### 3.2.3: Implementation Risk Assessment
```
Before proceeding with implementation:
- Estimate implementation complexity (1-5 scale)
- Identify potential breaking changes
- List required testing scenarios
- Calculate token/time investment

SAFETY CHECK: If complexity > 3 or breaking changes identified → Confirm with user before implementation
```

---

## Phase 4: Execution & Monitoring Chain
*Controlled implementation with continuous validation*

### Step 4.1: Incremental Implementation
```
Implement solution in atomic steps:
1. Make smallest possible change first
2. Validate change doesn't break existing functionality  
3. Test specific fix addresses target issue
4. Document change and reasoning
5. Proceed to next incremental change

CHECKPOINT: After each change, verify system still meets compliance requirements
```

### Step 4.2: Post-Implementation Validation
```
Systematic verification sequence:
1. Run automated tests if available
2. Check compliance with original constraints  
3. Verify no regression in previously working features
4. Document what was changed and why
5. Update relevant .claude/ files with learnings

FINAL VALIDATION: Can you explain to the user exactly what was changed and why it solves the problem?
```

---

## Error Recovery & Fallback Strategies

### Gemini Tool Errors
```
If Gemini command fails:
1. Check if paths in @ syntax exist and are accessible
2. Simplify query scope (fewer files/directories)  
3. Break complex queries into smaller parts
4. Fallback to native Claude analysis for smaller scopes
5. Document failure reason for user awareness
```

### Validation Failures
```
If validation checkpoints fail:
1. Re-examine assumptions about the problem
2. Gather additional context from user
3. Restart analysis chain with clearer constraints
4. Consider problem may need different approach entirely
5. Escalate to user rather than proceeding with uncertainty
```

### Implementation Failures
```
If implementation creates new issues:
1. Immediately revert changes
2. Re-analyze with focus on what was missed
3. Update problem understanding based on failure
4. Restart validation chain with new insights
5. Document failure patterns to avoid repetition
```

---

## Usage Guidelines

## AUTOMATIC PHASE SELECTION
Claude should automatically determine which phase to apply based on:

**Phase 1 Triggers**: New session, context changes, missing project understanding
**Phase 2 Triggers**: >50 files, codebase-wide analysis, >100KB content
**Phase 3 Triggers**: Complex implementations, performance-critical changes
**Phase 4 Triggers**: All code changes and implementations

**Decision Flow**: 
1. Assess task complexity and scope
2. Check if previous phases completed successfully
3. Apply appropriate phase automatically
4. Inform user which phase is being executed and why

### Success Metrics:
- **Validation Pass Rate**: >90% of checkpoints should pass on first attempt
- **Error Prevention**: <10% of implementations should require rollback
- **User Clarity**: User should understand reasoning for 100% of decisions
- **Compliance**: 100% adherence to constraints from compliance.txt

### Continuous Improvement:
- Document patterns that consistently work
- Track common failure points in validation
- Update atomic prompts based on recurring issues
- Refine Gemini query patterns for better results

---

## Key Improvements in This Implementation:

✅ **Atomic Prompts**: Each step does exactly one thing well
✅ **Validation Checkpoints**: Built-in verification after major steps  
✅ **Error Prevention**: Systematic checks before expensive operations
✅ **Progressive Disclosure**: Complexity introduced gradually
✅ **Modular Design**: Reusable components for common patterns
✅ **Fallback Strategies**: Clear recovery paths when things go wrong
✅ **Success Measurement**: Concrete metrics for system effectiveness
✅ **Documentation**: Every decision point includes reasoning

This approach transforms the agent from reactive troubleshooting to proactive, systematic problem-solving with built-in reliability measures.