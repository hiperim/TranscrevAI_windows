Project Overview
You are assisting with TranscrevAI, an AI-powered real-time transcription platform with advanced speaker diarization. This project provides high-accuracy speech-to-text services using state-of-the-art AI models with real-time processing capabilities.

Key Project Components

Core Functionality
• Real-time Audio Transcription: Convert speech to text using OpenAI Whisper models with superior accuracy
• Multi-language Support: Handle multiple languages (English, Spanish, Brazilian Portuguese) with expandable language support
• Neural Speaker Diarization: Advanced speaker identification using PyAnnote.Audio for precise speaker separation
• Live Processing: Concurrent transcription and diarization with real-time streaming results
• Timestamp Integration: Word-level timing synchronization between transcription and speaker segments

Technical Stack
• Backend: Python-based with FastAPI framework
• AI/ML Models: OpenAI Whisper (local models) + PyAnnote.Audio for neural diarization
• Real-time Communication: WebSocket-based streaming for live updates
• Audio Processing: Advanced preprocessing with LUFS normalization and noise reduction
• Frontend: HTML/CSS/JavaScript (planned upgrade to React/Vue.js)
• File Management: Cross-platform atomic operations with secure path validation

Current Architecture Transition
FROM (Legacy):
• VOSK-based speech recognition (being completely removed)
• Sequential processing (transcription → diarization)
• Basic frequency-based speaker detection
• Batch-only processing workflow

TO (Target Implementation):
• OpenAI Whisper local small english, spanish, and brazilian portuguese models with automatic download management
• Parallel processing architecture (concurrent transcription + diarization)
• PyAnnote.Audio neural diarization with voice embeddings
• Real-time streaming with live preview capabilities
• Dynamic model management with language-specific downloads

Key Features Being Implemented

1. Whisper Integration & Small Models Management
• Complete removal of VOSK dependencies and model downloads
• Local Whisper small models integration with automatic download system
• Language selector with "[Select Language]" default state
• Progressive download with real-time status updates ("Please wait for download to finish")
• Model validation and caching system

2. Parallel Processing Architecture
• Concurrent transcription and diarization processing
• Shared audio stream distribution
• Real-time result synchronization
• Performance optimization for live processing

3. Advanced Neural Diarization
• PyAnnote.Audio integration replacing frequency-based methods
• Voice embedding generation for speaker identification
• Adaptive speaker counting and verification
• Enhanced temporal alignment algorithms

4. Real-time User Experience
• Live transcription streaming via WebSocket
• Dynamic language selection with model auto-download
• Real-time processing status and progress indicators
• Seamless model switching and management

Technical Implementation Priorities

Phase 1 - Core Model Transition
✓ Cross-platform compatibility (Windows/macOS/Linux)
✓ Remove all VOSK references from codebase
✓ Implement Whisper small model management system
✓ Create dynamic language selector with download workflow
✓ Establish parallel processing foundation
✓ Integrate PyAnnote.Audio diarization pipeline

Phase 2 - Real-time Enhancement
• WebSocket streaming optimization
• Live transcription preview
• Performance monitoring and optimization
• Advanced error handling and recovery

Phase 3 - Advanced Features
• Multi-format export capabilities
• Enhanced audio preprocessing
• Mobile-responsive interface
• API development for third-party integration

Current Development Context

Active Implementation Tasks:
1. **Model System Overhaul**: Replacing VOSK with Whisper throughout the application
2. **Parallel Architecture**: Implementing concurrent processing for transcription and diarization
3. **Neural Diarization**: Integrating PyAnnote.Audio for superior speaker identification
4. **Dynamic Downloads**: Creating user-initiated small models download system with progress tracking
5. **Real-time Streaming**: WebSocket-based live transcription and diarization results

Key Files Being Modified:
• main.py: Model management, WebSocket handlers, parallel processing coordination
• transcription.py: Whisper integration replacing VOSK
• speaker_diarization.py: PyAnnote.Audio neural diarization implementation
• app_config.py: Updated configuration for new model system
• file_manager.py: Whisper small models download and management

Technical Requirements & Constraints

Performance Targets:
• Real-time processing capability (1:1 or better processing ratio)
• Concurrent transcription and diarization without blocking
• Automatic model download under 2 minutes for small models
• WebSocket latency under 100ms for live updates

Model Requirements:
• Local Whisper model support (small variants)
• PyAnnote.Audio pipeline integration
• Language-specific model downloading
• Model caching and validation system

Architecture Principles:
• Concurrent processing design patterns
• Atomic operations for file management
• Graceful error handling with fallback mechanisms
• Resource-efficient model loading/unloading
• Cross-platform compatibility (Windows/macOS/Linux)

Primary Use Cases: 
• Live meeting transcription with speaker identification
• Content creation workflow (podcasts, interviews, videos)
• Research and analysis requiring speaker-separated transcripts
• Accessibility applications for hearing impaired users

Competitive Advantages:
• Local processing (privacy and security)
• Real-time concurrent processing
• Superior accuracy through Whisper + PyAnnote.Audio combination
• No subscription fees for core functionality
• Professional-grade audio preprocessing

Communication Preferences

When discussing TranscrevAI development:
• Focus on concurrent processing implementation details
• Prioritize real-time performance and user experience
• Emphasize model transition from VOSK to Whisper
• Consider PyAnnote.Audio integration challenges
• Consider parallel processing integration challenges
• Address WebSocket streaming optimization
• Suggest testing strategies for real-time systems

Decision-Making Framework

For technical implementation decisions:
• **Real-time Performance**: Does this maintain or improve processing speed?
• **Model Accuracy**: Does this leverage the best available AI models (Whisper + PyAnnote)?
• **User Experience**: Does this provide immediate feedback and smooth workflow?
• **Concurrent Design**: Does this support parallel processing architecture?
• **Resource Efficiency**: Does this optimize memory and CPU usage?
• **Maintainability**: Is this approach sustainable for future enhancements?

Current Implementation Focus

You are currently helping implement the transition from VOSK to Whisper with parallel processing and neural diarization. Key areas of focus:

1. **Clean VOSK Removal**: Eliminate all VOSK dependencies, model URLs, and download logic
2. **Whisper Integration**: Implement local Whisper small models download, management and transcription
3. **Neural Diarization**: Replace frequency-based methods with PyAnnote.Audio
4. **Parallel Processing**: Design concurrent transcription and diarization workflows
5. **Dynamic UX**: Create responsive language selection with download progress
6. **Real-time Streaming**: Optimize WebSocket communication for live results

Instructions for Claude: 
Always reference this updated context when discussing TranscrevAI. The project is in active transition from VOSK to Whisper with parallel processing implementation. Focus on practical code examples for concurrent processing, Whisper integration, and PyAnnote.Audio implementation. Prioritize real-time performance and user experience improvements. Ask specific questions about implementation details when clarification is needed.